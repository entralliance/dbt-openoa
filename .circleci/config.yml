
version: 2

jobs:

  integration-postgres:
    docker:
      - image: cimg/python:3.9.9
      - image: circleci/postgres:9.6.5-alpine-ram

    steps:
      - checkout
      - restore_cache:
          key: deps_postgres-{{ .Branch }}
      - run:
          name: "Set Up dbt"
          command: |
            python3 -m venv dbt_venv
            . dbt_venv/bin/activate
            pip install --upgrade pip setuptools
            pip install --pre dbt-postgres
            mkdir -p ~/.dbt
            cp integration_tests/ci/sample.profiles.yml ~/.dbt/profiles.yml
      - run:
          name: "Run Tests - Postgres"
          environment:
            POSTGRES_TEST_HOST: localhost
            POSTGRES_TEST_USER: root
            POSTGRES_TEST_PASS: ''
            POSTGRES_TEST_PORT: 5432
            POSTGRES_TEST_DBNAME: circle_test
          command: |
            . dbt_venv/bin/activate
            cd integration_tests
            dbt --warn-error deps --target postgres
            dbt --warn-error seed --target postgres --full-refresh
            dbt --warn-error run --target postgres --full-refresh
            dbt --warn-error test --target postgres
      - save_cache:
          key: deps_postgres-{{ .Branch }}
          paths:
            - "dbt_venv"
      - store_artifacts:
          path: ./logs


  integration-snowflake:
    docker:
      - image: cimg/python:3.9.9

    steps:
      - checkout
      - restore_cache:
          key: deps_snowflake-{{ .Branch }}
      - run:
          name: "Set Up dbt"
          command: |
            python3 -m venv dbt_venv
            . dbt_venv/bin/activate
            pip install --upgrade pip setuptools
            pip install --pre dbt-snowflake
            mkdir -p ~/.dbt
            cp integration_tests/ci/sample.profiles.yml ~/.dbt/profiles.yml
      - run:
          name: "Run Tests - Snowflake"
          command: |
            . dbt_venv/bin/activate
            echo `pwd`
            cd integration_tests
            dbt --warn-error deps --target snowflake
            dbt --warn-error seed --target snowflake --full-refresh
            dbt --warn-error run --target snowflake --full-refresh
            dbt --warn-error test --target snowflake
      - save_cache:
          key: deps_snowflake-{{ .Branch }}
          paths:
            - "dbt_venv"
      - store_artifacts:
          path: ./logs


  integration-bigquery:
    docker:
      - image: cimg/python:3.9.9
    
    steps:
      - checkout
      - run:
          run: setup_creds
          command: |
            echo $BIGQUERY_SERVICE_ACCOUNT_JSON > ${HOME}/bigquery-service-key.json
      - restore_cache:
          key: deps_bigquery-{{ .Branch }}
      - run:
          name: "Set Up dbt"
          command: |
            python3 -m venv dbt_venv
            . dbt_venv/bin/activate
            pip install --upgrade pip setuptools
            pip install --pre dbt-bigquery
            mkdir -p ~/.dbt
            cp integration_tests/ci/sample.profiles.yml ~/.dbt/profiles.yml
      - run:
          name: "Run Tests - BigQuery"
          environment:
              BIGQUERY_SERVICE_KEY_PATH: "/home/circleci/bigquery-service-key.json"
          command: |
            . dbt_venv/bin/activate
            echo `pwd`
            cd integration_tests
            dbt --warn-error deps --target bigquery
            dbt --warn-error seed --target bigquery --full-refresh
            dbt --warn-error run --target bigquery --full-refresh
            dbt --warn-error test --target bigquery
      - save_cache:
          key: deps_bigquery-{{ .Branch }}
          paths:
            - "dbt_venv"
      - store_artifacts:
          path: ./logs


  integration-spark:
    docker:
      - image: cimg/python:3.9.9
      - image: godatadriven/spark:3.3
        environment:
          WAIT_FOR: localhost:5432
        command: >
          --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2
          --name Thrift JDBC/ODBC Server
          --conf spark.hadoop.javax.jdo.option.ConnectionURL=jdbc:postgresql://localhost/metastore
          --conf spark.hadoop.javax.jdo.option.ConnectionUserName=dbt
          --conf spark.hadoop.javax.jdo.option.ConnectionPassword=dbt
          --conf spark.hadoop.javax.jdo.option.ConnectionDriverName=org.postgresql.Driver
          --conf spark.hadoop.datanucleus.autoCreateSchema=true
          --conf spark.hadoop.datanucleus.autoCreateTables=true
          --conf spark.hadoop.datanucleus.fixedDatastore=false
          --conf spark.hive.metastore.schema.verification=false
      - image: postgres:9.6.17-alpine
        environment:
          POSTGRES_USER: dbt
          POSTGRES_PASSWORD: dbt
          POSTGRES_DB: metastore

    steps:
      - checkout
      - restore_cache:
          key: deps_spark-{{ .Branch }}
      - run:
          name: Wait for Spark-Thrift
          command: dockerize -wait tcp://localhost:10000 -timeout 15m -wait-retry-interval 5s
      - run:
          name: "Set Up dbt"
          command: |
            sudo apt-get update
            sudo apt-get install libsasl2-dev gcc
            python3 -m venv dbt_venv
            . dbt_venv/bin/activate
            pip install --upgrade pip setuptools
            pip install dbt-spark[PyHive]
            mkdir -p ~/.dbt
            cp integration_tests/ci/sample.profiles.yml ~/.dbt/profiles.yml
      - run:
          name: "Run Tests - Spark"
          environment:
            SPARK_TEST_HOST: localhost
            SPARK_TEST_USER: dbt
            SPARK_TEST_PORT: 10000
          command: |
            . dbt_venv/bin/activate
            cd integration_tests_spark
            dbt --warn-error deps --target spark
            dbt --warn-error seed --target spark --full-refresh
            dbt --warn-error run --target spark --full-refresh
            dbt --warn-error test --target spark
      - save_cache:
          key: deps_spark-{{ .Branch }}
          paths:
            - "dbt_venv"
      - store_artifacts:
          path: ./logs

      # - run:
      #     name: "Run Tests - Redshift"
      #     command: |
      #       . dbt_venv/bin/activate
      #       echo `pwd`
      #       cd integration_tests
      #       dbt --warn-error deps --target redshift
      #       dbt --warn-error seed --target redshift --full-refresh
      #       dbt --warn-error run --target redshift --full-refresh --vars 'update: false'
      #       dbt --warn-error run --target redshift --vars 'update: true'
      #       dbt --warn-error test --target redshift

workflows:
  version: 2
  warehouse-integration-tests:
    jobs:
      - integration-postgres
      - integration-snowflake:
          context:
            - profile-snowflake
      - integration-bigquery:
          context:
            - profile-bigquery
      - integration-spark
